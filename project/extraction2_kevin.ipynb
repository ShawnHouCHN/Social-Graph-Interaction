{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 02805 Social graphs and interactions **\n",
    "\n",
    "# Data Extraction Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "# IPython global cell magic\n",
    "%reset\n",
    "%matplotlib inline\n",
    "\n",
    "# import all necessary packages\n",
    "import bs4 # HTML parser\n",
    "from collections import Counter, OrderedDict # counting elements and ordering keys in dictionaries\n",
    "import community # python-louvain package\n",
    "from __future__ import division # all numbers are float\n",
    "import datetime # handle date objects\n",
    "import dateparser # parse any (also foreign) date format to object: https://pypi.python.org/pypi/dateparser\n",
    "import itertools # iterators for efficient looping\n",
    "import json # JSON parser\n",
    "import math # math operations\n",
    "from matplotlib import pyplot as plt # plotting figures\n",
    "import mwparserfromhell # parse MediaWiki syntax: https://github.com/earwig/mwparserfromhell\n",
    "from nameparser import HumanName # parse a human name\n",
    "import networkx as nx # networks creation library\n",
    "import nltk # natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import operator # efficient operator functions\n",
    "import os # operating system operations, e.g.: with files and folders\n",
    "import pandas as pd # use easy-to-use data frames for data analysis\n",
    "import pickle # python data structures as files\n",
    "from pprint import pprint # print data structures prettier\n",
    "import re # regex\n",
    "import requests # request URL content\n",
    "from tqdm import tqdm_notebook # make a nice progressbar\n",
    "import urllib # handle special URL chars\n",
    "\n",
    "# make working directory\n",
    "directory = os.getcwd() + '/companies'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# files\n",
    "ex1_fdat = directory + '/extraction1_data.pkl'\n",
    "ex2_fdat = directory + '/extraction2_data.pkl'\n",
    "ex3_tmp_fdat = directory + '/tmp_extraction3_data.pkl'\n",
    "ex3_fdat = directory + '/extraction3_data.pkl'\n",
    "merged = directory + '/merged_data.pkl'\n",
    "extraction_csv = directory + '/company_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_company_pages(url, companies):\n",
    "    # request first page\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'lxml')\n",
    "    # find all li elements in ul with links\n",
    "    table = soup.find(\"table\", attrs={\"class\": \"wikitable\"})\n",
    "    rows = table.find_all(\"tr\")\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if cells:\n",
    "            a = cells[1].find('a', href=True)\n",
    "            wiki_name = a.get_text()\n",
    "            companies[wiki_name] = {\n",
    "                'wiki_name': wiki_name,\n",
    "                'wiki_url': a['href'],\n",
    "                'name_url_quoted': urllib.quote_plus(wiki_name.encode('utf-8'))}\n",
    "\n",
    "    # request more pages in recursive loop following \"next 1000 entries\" link\n",
    "    content = soup.find(\"div\", attrs={\"id\": \"content\"})\n",
    "    next_link = content.find('a', href=True, text='Next 1000 entries')\n",
    "    if next_link:\n",
    "        companies = get_company_pages(next_link['href'], companies)\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47858 companies were extracted.\n"
     ]
    }
   ],
   "source": [
    "# parse HTML\n",
    "wiki_company_articles = u'https://tools.wmflabs.org/enwp10/cgi-bin/list2.fcgi?run=yes&projecta=Company&limit=1000'\n",
    "\n",
    "companies = get_company_pages(wiki_company_articles, dict())\n",
    "print len(companies), \"companies were extracted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from first extraction\n",
    "extract1 = dict()\n",
    "if os.path.isfile(ex1_fdat):\n",
    "    with open(ex1_fdat, 'rb') as f:\n",
    "        extract1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/w/index.php?title=AT%26T\n",
      "https://en.wikipedia.org/w/index.php?title=AT%26T\n"
     ]
    }
   ],
   "source": [
    "company = 'AT&T'\n",
    "print companies[company]['wiki_url']\n",
    "\n",
    "if extract1 and company in extract1:\n",
    "    print extract1[company]['wiki_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_json_from_url(url):\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # on HTML error codes\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    # try converting into JSON\n",
    "    try:\n",
    "        sec = r.json()\n",
    "        return sec\n",
    "    except ValueError:  # includes simplejson.decoder.JSONDecodeError\n",
    "        print 'WARN: Decoding JSON has failed on:', url\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_date(date):\n",
    "    if date:\n",
    "        dateparser.parse(date, settings={\n",
    "            'PREFER_DATES_FROM': 'past',\n",
    "            'DATE_ORDER': 'YMD'})\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_c_info(link_url, all_companies, company):\n",
    "    \"\"\"Get wiki links, verify company existance and get additional data\"\"\"\n",
    "    \n",
    "    c_content = get_json_from_url(link_url)\n",
    "\n",
    "    if not c_content and 'parse' not in c_content:\n",
    "        print 'No parsable content on:', link_url\n",
    "        company['is_company'] = False\n",
    "        return company\n",
    "\n",
    "    # add certain fields\n",
    "    company['wiki_page_id'] = c_content['parse']['pageid']\n",
    "    company['wiki_name'] = c_content['parse']['title']\n",
    "    company['wiki_api_url'] = link_url\n",
    "    # list of links intersected with list containing all companies\n",
    "    links = [x['*'] for x in c_content['parse']['links'] if x['ns'] == 0]\n",
    "    company['all_links'] = links\n",
    "    company['links'] = all_companies.intersection(links)\n",
    "    company['wiki_raw'] = c_content['parse']['wikitext']['*']\n",
    "    \n",
    "    # check company on OpenCorporates to verify existance\n",
    "    # see: https://api.opencorporates.com/documentation/API-Reference\n",
    "    # example: https://api.opencorporates.com/v0.4/companies/search?q=Anglo-Persian%20Oil%20Company\n",
    "    oc_api_base = 'https://api.opencorporates.com/v0.4/companies'\n",
    "    # normalise_company_name=true - \n",
    "    # order=score - sort after score not alphabetic\n",
    "    oc_properties = '&normalise_company_name=true&order=score'\n",
    "    company['oc_api_url'] = '{0}/search?q={1}{2}'.format(oc_api_base, company['name_url_quoted'], oc_properties)\n",
    "        \n",
    "    # get data\n",
    "    oc_resp = get_json_from_url(company['oc_api_url'])  \n",
    "    # take the first company with highest score\n",
    "    company_data = oc_resp['results']['companies']\n",
    "    # return if there are no results for the company\n",
    "    if not company_data:\n",
    "        company['is_company'] = False\n",
    "        return company\n",
    "    \n",
    "    comp = company_data[0]['company']\n",
    "    # set all the fields that match with what can be gathered from Infobox company template\n",
    "    company['name'] = comp['name']\n",
    "    company['type'] = comp['company_type']   \n",
    "    company['defunct'] = parse_date(comp['dissolution_date'])\n",
    "    company['founded'] = parse_date(comp['incorporation_date'])\n",
    "    company['location_country'] = comp['registered_address']['country']\n",
    "    company['location_city'] = comp['registered_address']['locality']\n",
    "    company['location'] = comp['registered_address']['street_address']\n",
    "\n",
    "    # look if there is any network data on OC\n",
    "    # difficult because search for BP does not necessarily return the right company\n",
    "    # also OC has network data only for a very small subset\n",
    "    company['oc_api_network_url'] = '{0}/{1}/{2}/network'.format(\n",
    "        oc_api_base, comp['jurisdiction_code'], comp['company_number'])    \n",
    "    c_network = get_json_from_url(company['oc_api_network_url'])\n",
    "    if c_network['results']:\n",
    "        print \"Company network found for:\", company['wiki_name'], company['oc_api_network_url']\n",
    "\n",
    "    # discard if this company was already processed\n",
    "    if extract1 and \\\n",
    "        company['wiki_name'] in extract1 and \\\n",
    "        'is_company' in extract1[company['wiki_name']] and \\\n",
    "        extract1[company['wiki_name']]['is_company']:\n",
    "            company['is_company'] = False\n",
    "            return company\n",
    "    # otherwise save it as new company\n",
    "    company['is_company'] = True\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total there are 82988 unprocessed companies.\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/w/api.php?action=parse&page=Audi&prop=links|wikitext\n",
    "wiki_base = u'https://en.wikipedia.org'\n",
    "wiki_api = u'/w/api.php'\n",
    "action = u'action=parse'\n",
    "dat_format = u'format=json'\n",
    "properties = u'prop=links|wikitext'\n",
    "\n",
    "# other: 'Audi', 'Apple Inc.', 'Microsoft'\n",
    "company = 'AT&T'\n",
    "link_url = u'{0}{1}?{2}&{3}&{4}&page={5}'.format(\n",
    "    wiki_base, wiki_api, action, dat_format, properties, urllib.quote_plus(company.encode('utf-8')))\n",
    "\n",
    "# full set of companies is from extract1 and companies combined\n",
    "all_companies = set(companies.keys() + extract1.keys())\n",
    "print \"In total there are {0} companies ({1} processed and {2} unprocessed).\".format(\n",
    "    len(all_companies), len(companies.keys), len(extract1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt of AT&T dict structure:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'is_company'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-6a48d2368460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Excerpt of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dict structure:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_c_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_companies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-1ade48affb86>\u001b[0m in \u001b[0;36mget_c_info\u001b[0;34m(link_url, all_companies, company)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# discard if this company was already processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mextract1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompany\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wiki_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextract1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mextract1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wiki_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_company'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mcompany\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_company'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompany\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_company'"
     ]
    }
   ],
   "source": [
    "print \"Excerpt of\", company, \"dict structure:\"\n",
    "pprint(get_c_info(link_url, all_companies, companies[company]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### now check if new links must be added to companies from extraction 1\n",
    "\n",
    "# get all companies where 'is_company' is true\n",
    "\n",
    "# merge extraction1 and extraction2 (from extraction2 oc fields can be added)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
