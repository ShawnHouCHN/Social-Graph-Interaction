{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 02805 Social graphs and interactions **\n",
    "\n",
    "# Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# IPython global cell magic\n",
    "%reset\n",
    "%matplotlib inline\n",
    "\n",
    "# import all necessary packages\n",
    "import bs4 # HTML parser\n",
    "from collections import Counter, OrderedDict # counting elements and ordering keys in dictionaries\n",
    "import community # python-louvain package\n",
    "from __future__ import division # all numbers are float\n",
    "import geopy # get geo location according to addresses\n",
    "import datetime # handle date objects\n",
    "import dateparser # parse any (also foreign) date format to object: https://pypi.python.org/pypi/dateparser\n",
    "import itertools # iterators for efficient looping\n",
    "import json # JSON parser\n",
    "import math # math operations\n",
    "from matplotlib import pyplot as plt # plotting figures\n",
    "import mwparserfromhell # parse MediaWiki syntax: https://github.com/earwig/mwparserfromhell\n",
    "from nameparser import HumanName # parse a human name\n",
    "import networkx as nx # networks creation library\n",
    "import nltk # natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import operator # efficient operator functions\n",
    "import os # operating system operations, e.g.: with files and folders\n",
    "import pandas as pd # use easy-to-use data frames for data analysis\n",
    "import pickle # python data structures as files\n",
    "from pprint import pprint # print data structures prettier\n",
    "import re # regex\n",
    "import requests # request URL content\n",
    "import time # sleep timer\n",
    "from tqdm import tqdm_notebook # make a nice progressbar\n",
    "import urllib # handle special URL chars\n",
    "\n",
    "# make working directory\n",
    "directory = os.getcwd() + '/companies'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# files\n",
    "ex1_fdat = directory + '/extraction1_data.pkl'\n",
    "ex2_fdat = directory + '/extraction2_data.pkl'\n",
    "ex3_tmp_fdat = directory + '/tmp_extraction3_data.pkl'\n",
    "ex3_fdat = directory + '/extraction3_data.pkl'\n",
    "merged = directory + '/merged_data.pkl'\n",
    "extraction_csv = directory + '/company_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from all extractions\n",
    "com_dat = dict()\n",
    "if os.path.isfile(merged):\n",
    "    with open(merged, 'rb') as f:\n",
    "        com_dat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_employee_nr(input_val, c_name):\n",
    "    \"\"\"\n",
    "    Parse a proper number out of the various inputs\n",
    "    Case possibilities tested:\n",
    "    \n",
    "    s_list = [\n",
    "        '15-50',\n",
    "        '~300+',\n",
    "        'circa 40',\n",
    "        '9,985(Dec 2011)',\n",
    "        'over 10,000 in 10 countries',\n",
    "        'Five',\n",
    "        'over 1 million',\n",
    "        '10.000',\n",
    "        'Part of Popular, Inc., which has 8,000 employees'\n",
    "    ]\n",
    "\n",
    "    for s in s_list:\n",
    "        print parse_employee_nr(s, 'test')\n",
    "    \"\"\"\n",
    "    \n",
    "    # match the first number, dot or comma separation optional\n",
    "    m = re.search(r'[0-9]+([,\\.][0-9]+)?', unicode(input_val))\n",
    "    if m:\n",
    "        try:\n",
    "            # replace , and conert to int\n",
    "            return int(m.group().replace(',', '').replace('.', ''))\n",
    "        except ValueError:\n",
    "            print \"WARN: Failed conversion of:{0} (company: {1})\".format(\n",
    "                input_val, c_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify the type for each field\n",
    "types = {\n",
    "    # when first link is crawled\n",
    "    'wiki_name': unicode,\n",
    "    'wiki_url': str,\n",
    "    'name_url_quoted': str,\n",
    "    \n",
    "    # when WIKI API is crawled\n",
    "    'wiki_page_id': float,\n",
    "    'wiki_api_url': str,\n",
    "    'all_links': list,\n",
    "    'links': set,\n",
    "    'is_company': bool,\n",
    "    'wiki_raw': unicode,\n",
    "    \n",
    "    # from extraction2\n",
    "    'oc_api_url': str,\n",
    "    'oc_api_search_url': str,\n",
    "    'oc_api_network_url': str,\n",
    "    \n",
    "    # added only when Infobox company exists or fields from OpenCorporates\n",
    "    # not all fields always exist, they are NaN in the resulting DataFrame\n",
    "    'name': unicode, \n",
    "    'type': unicode, \n",
    "    'founded': datetime.datetime, \n",
    "    'defunct': datetime.datetime, \n",
    "    'location': unicode,\n",
    "    'location_city': unicode,\n",
    "    'location_country': unicode,\n",
    "    'location_geopy': unicode,\n",
    "    'location_gps': tuple,\n",
    "    # following not in OC\n",
    "    'countries': set, # added with extraction 3\n",
    "    'logo': dict,\n",
    "    'key_people': list, # additonally processed with nameparser.HumanName (dict)\n",
    "    'industry': list, \n",
    "    'subsid': list,\n",
    "    'products': list, \n",
    "    'num_employees': float, \n",
    "    'parent': unicode, \n",
    "    'homepage': unicode\n",
    "}\n",
    "\n",
    "for c, c_dict in com_dat.iteritems():\n",
    "    for k, val in c_dict.iteritems():\n",
    "        if val and not isinstance(types[k], list) and isinstance(val, list):\n",
    "            if k == 'num_employees':\n",
    "                com_dat[c][k] = parse_employee_nr(val[0], c)\n",
    "            else:\n",
    "                com_dat[c][k] = val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert from dict into dataframe\n",
    "comp_df = pd.DataFrame.from_dict(com_dat, orient='index', dtype=set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as CSV file\n",
    "comp_df.to_csv(extraction_csv, encoding='utf-8', index_label='wiki_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_dat = pd.read_csv(extraction_csv, index_col=0, dtype=types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "for c, row in c_dat.iterrows():\n",
    "    print type(row['links'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies with most links:\n",
      "[('China State Construction Engineering Corporation ', 2540),\n",
      " ('China Communications Construction', 2538),\n",
      " ('Metallurgical Corporation of China Limited', 2518),\n",
      " ('Dexia Crediop', 2490),\n",
      " ('Banca CRS', 2489),\n",
      " ('IW BANK S.p.A.', 2484),\n",
      " ('Banca IFIS S.p.A.', 2480),\n",
      " ('Banca Carim', 2479),\n",
      " ('ChiantiBanca', 2478),\n",
      " ('Fidi Toscana', 2478)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "# iterate every row that represents a company\n",
    "for index, row in c_dat.iterrows():\n",
    "    # empty link list\n",
    "    if isinstance(row['links'], float):\n",
    "        continue\n",
    "    cnt[row['name']] = len(row['links'])\n",
    "\n",
    "print \"Companies with most links:\"\n",
    "pprint(cnt.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most companies by country:\n",
      "[('United Bank Limited (UBL)', 15000.0),\n",
      " ('Taiwan Sugar Corporation', nan),\n",
      " (nan, 7000.0),\n",
      " (\"Valle's Steak House\", 3600.0),\n",
      " ('Nammo AS', 1900.0),\n",
      " ('Framo-Werke GmbH', 1000.0),\n",
      " ('Irish Bank Resolution Corporation', 850.0),\n",
      " ('Blue Nile Inc.', 301.0),\n",
      " ('Nordcurrent', 80.0),\n",
      " ('Serellan', nan)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "for index, row in c_dat.iterrows():\n",
    "    cnt[row['name']] = row['num_employees']\n",
    "\n",
    "print \"Most companies by country:\"\n",
    "pprint(cnt.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct the Company Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one node per company name (keys of data)\n",
    "c_graph = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&pizza {\n",
      "&pizza R\n",
      "&pizza u\n",
      "&pizza b\n",
      "&pizza y\n",
      "&pizza  \n",
      "&pizza T\n",
      "&pizza u\n",
      "&pizza e\n",
      "&pizza s\n",
      "&pizza d\n",
      "&pizza a\n",
      "&pizza y\n",
      "&pizza  \n",
      "&pizza (\n",
      "&pizza r\n",
      "&pizza e\n",
      "&pizza s\n",
      "&pizza t\n",
      "&pizza a\n",
      "&pizza u\n",
      "&pizza r\n",
      "&pizza a\n",
      "&pizza n\n",
      "&pizza t\n",
      "&pizza )\n",
      "&pizza ,\n",
      "&pizza  \n",
      "&pizza C\n",
      "&pizza i\n",
      "&pizza t\n",
      "&pizza y\n",
      "&pizza  \n",
      "&pizza S\n",
      "&pizza p\n",
      "&pizza o\n",
      "&pizza r\n",
      "&pizza t\n",
      "&pizza s\n",
      "&pizza }\n"
     ]
    }
   ],
   "source": [
    "# create nodes and edges\n",
    "for c, row in c_dat.iterrows():\n",
    "\n",
    "    # don't add non companies\n",
    "    if 'is_company' not in row or not row['is_company']:\n",
    "        continue\n",
    "\n",
    "    # add a node for the company\n",
    "    c_graph.add_node(\n",
    "        c,\n",
    "        name=row['name'], \n",
    "        type=row['type'],\n",
    "        key_people=row['key_people'],\n",
    "        industry=row['industry'],\n",
    "        founded=row['founded'],\n",
    "        #location_geopy=row['location_geopy'],\n",
    "        location_city=row['location_city'],\n",
    "        location_country=row['location_country'],\n",
    "        #location_gps=row['location_gps'],\n",
    "        defunct=row['defunct'],\n",
    "        subsid=row['subsid'],\n",
    "        products=row['products'],\n",
    "        num_employees=row['num_employees'],\n",
    "        parent=row['parent']\n",
    "    )\n",
    "\n",
    "    # show example of related company\n",
    "    if c == 'Apple Inc.':\n",
    "        print row['links']    \n",
    "    \n",
    "    # check if edge list exists\n",
    "    if 'links' not in row:\n",
    "        print 'No link list for company:', c\n",
    "        continue\n",
    "\n",
    "    # add an edge for the company\n",
    "    for e in row['links']:\n",
    "        print c, e\n",
    "        c_graph.add_edge(c, e)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 nodes in the network.\n",
      "There are 0 edges in the network.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {0} nodes in the network.\".format(len(p_graph.nodes()))\n",
    "print \"There are {0} edges in the network.\".format(len(p_graph.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weakly Connected Component is maximal subgraph of a directed graph such that for every pair of vertices $u$, $v$ in the subgraph, there is an undirected path from $u$ to $v$ and a directed path from $v$ to $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_weak = sorted(nx.weakly_connected_component_subgraphs(p_graph), key=len, reverse=True)[0]\n",
    "print 'The size (number of nodes) of subgraph with largest weakly connected component is:', p_weak.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### degree centrality\n",
    "# in-edges\n",
    "node_in_degree = nx.in_degree_centrality(p_weak)\n",
    "# out-edges\n",
    "node_out_degree = nx.out_degree_centrality(p_weak)\n",
    "\n",
    "print \"5 most central companies according to in-edges degree centrality:\"\n",
    "pprint.pprint(Counter(node_in_degree).most_common(5))\n",
    "\n",
    "print \"\\n5 most central companies according to out-edges degree centrality:\"\n",
    "pprint.pprint(Counter(node_out_degree).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate betweenness centrality\n",
    "node_dict = nx.betweenness_centrality(p_weak)\n",
    "\n",
    "# use to print with betweenness_centrality\n",
    "print \"Top 5 most central company according to betweenness centrality:\"\n",
    "pprint.pprint(Counter(node_dict).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### eigenvector centrality\n",
    "# in-edges\n",
    "node_in_eigen = nx.eigenvector_centrality(p_weak)\n",
    "# For out-edges eigenvector centrality first reverse the graph with G.reverse().\n",
    "node_out_eigen = nx.eigenvector_centrality(p_weak.reverse())\n",
    "\n",
    "print \"5 most central companies according to in-edges eigenvector centrality:\"\n",
    "pprint.pprint(Counter(node_in_eigen).most_common(5))\n",
    "\n",
    "print \"\\n5 most central companies according to out-edges eigenvector centrality:\"\n",
    "pprint.pprint(Counter(node_out_eigen).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What if?\n",
    "\n",
    "What if google disappears and all nodes connecting directly to google with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
