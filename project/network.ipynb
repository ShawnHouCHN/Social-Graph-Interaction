{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** 02805 Social graphs and interactions **\n",
    "\n",
    "# Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# IPython global cell magic\n",
    "%reset\n",
    "%matplotlib inline\n",
    "\n",
    "# import all necessary packages\n",
    "import bs4 # HTML parser\n",
    "from collections import Counter, OrderedDict # counting elements and ordering keys in dictionaries\n",
    "import community # python-louvain package\n",
    "import datetime # handle date objects\n",
    "import dateparser # parse any (also foreign) date format to object: https://pypi.python.org/pypi/dateparser\n",
    "from __future__ import division # all numbers are float\n",
    "import gc # garbage collector\n",
    "import geoplotlib # plot points on tiled maps\n",
    "from geoplotlib.utils import BoundingBox\n",
    "import geopy # get geo location according to addresses\n",
    "from geopy.exc import GeocoderServiceError\n",
    "from infomap import infomap # python infomap algorithm, needs to be in same directory\n",
    "import itertools # iterators for efficient looping\n",
    "import json # JSON parser\n",
    "import math # math operations\n",
    "from matplotlib import pyplot as plt # plotting figures\n",
    "import mwparserfromhell # parse MediaWiki syntax: https://github.com/earwig/mwparserfromhell\n",
    "from nameparser import HumanName # parse a human name\n",
    "import networkx as nx # networks creation library\n",
    "import nltk # natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import operator # efficient operator functions\n",
    "import os # operating system operations, e.g.: with files and folders\n",
    "import pandas as pd # use easy-to-use data frames for data analysis\n",
    "import pickle # python data structures as files\n",
    "from pprint import pprint # print data structures prettier\n",
    "import re # regex\n",
    "import requests # request URL content\n",
    "import sys # system operations\n",
    "import time # sleep timer\n",
    "from tqdm import tqdm_notebook # make a nice progressbar\n",
    "import urllib # handle special URL chars\n",
    "\n",
    "# make working directory\n",
    "directory = os.getcwd() + '/companies'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# files from data crawling\n",
    "ex1_fdat = directory + '/extraction1_data.pkl'\n",
    "ex2_fdat = directory + '/extraction2_data.pkl'\n",
    "ex3_tmp_fdat = directory + '/tmp_extraction3_data.pkl'\n",
    "ex3_fdat = directory + '/extraction3_data.pkl'\n",
    "merged = directory + '/merged_data.pkl'\n",
    "\n",
    "# network files\n",
    "network_f = directory + '/network.pkl'\n",
    "network_red_f = directory + '/reduced_network.pkl'\n",
    "gephi_f = directory + '/gehpi.gexf'\n",
    "\n",
    "# specify nltk data dir, otherwise LookupError\n",
    "nltk.data.path.append(os.getcwd() + '/../nltk_data')\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct the Company Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_dat = dict()\n",
    "# load network data if available\n",
    "if os.path.isfile(network_f):\n",
    "    with open(network_f, 'rb') as f:\n",
    "        net_dat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_graph(com_dat):\n",
    "\n",
    "    # init directed graph object\n",
    "    c_graph = nx.DiGraph()\n",
    "    \n",
    "    # iterate through company data\n",
    "    for c_name, comp in com_dat.iteritems():\n",
    "        \n",
    "        # don't add non companies\n",
    "        if 'is_company' not in comp or not comp['is_company']:\n",
    "            continue\n",
    "            \n",
    "        # check the coordinates\n",
    "        lat = 0.0\n",
    "        lng = 0.0\n",
    "        if 'location_gps' in comp:\n",
    "            if comp['location_gps'][0]:\n",
    "                lat = comp['location_gps'][0]\n",
    "            if comp['location_gps'][1]:\n",
    "                lng = comp['location_gps'][1]\n",
    "                \n",
    "        # convert the date to a string\n",
    "        founded = 'NaN'\n",
    "        defunct = 'NaN'\n",
    "        if 'founded' in comp and comp['founded']:\n",
    "            founded = comp['founded'].strftime('%Y-%m-%d')\n",
    "        if 'defunct' in comp and comp['defunct']: \n",
    "            defunct = comp['defunct'].strftime('%Y-%m-%d')\n",
    "            \n",
    "        # number of employees\n",
    "        num_employees = 0\n",
    "        if 'num_employees' in comp and comp['num_employees']:\n",
    "            num_employees = comp['num_employees']\n",
    "            \n",
    "        # just first element for lists\n",
    "        _type = 'NaN'\n",
    "        if 'num_employees' in comp and comp['num_employees']:\n",
    "            _type = comp['type'][0]\n",
    "        industry = 'NaN'\n",
    "        if 'industry' in comp and comp['industry']:\n",
    "            industry = comp['industry'][0]\n",
    "\n",
    "        # create one node per company name (keys of data)\n",
    "        c_graph.add_node(\n",
    "            # only attriutes where data interferes\n",
    "            c_name,\n",
    "            name=comp['name'], \n",
    "            type=_type, \n",
    "            industry=comp['industry'][0],\n",
    "            founded=founded,\n",
    "            defunct=defunct,\n",
    "            location_city=comp['location_city'],\n",
    "            location_country=comp['location_country'],\n",
    "            latitude=lat,\n",
    "            longitude=lng,\n",
    "            num_employees=num_employees,\n",
    "            parent=comp['parent']\n",
    "        )\n",
    "\n",
    "        # show example of related company\n",
    "        if c_name == 'Apple Inc.':\n",
    "            print \"Link list of\", c_name, \"company:\\n\", comp['links']\n",
    "\n",
    "        # add an edge for the company, but from the original dictionary\n",
    "        for e in comp['links']:\n",
    "            if e in com_dat and 'is_company' in com_dat[e] and com_dat[e]['is_company']:\n",
    "                c_graph.add_edge(c_name, e)\n",
    "                \n",
    "    return c_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nordisk Mobiltelefon (Sweden)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9742b770e0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc_graph_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_companies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-7ed1f81236e9>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m(com_dat)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mfounded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'founded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'defunct'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'defunct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdefunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'defunct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# number of employees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "c_graph_reduced = create_graph(merged_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_companies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9adef2e0a270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mc_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_dat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_dat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mc_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_companies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_companies' is not defined"
     ]
    }
   ],
   "source": [
    "if 'graph' in net_dat and net_dat['graph']:\n",
    "    c_graph =  net_dat['graph']\n",
    "elif 'raw_dat' in net_dat and net_dat['raw_dat']:\n",
    "    c_graph = create_graph(net_dat['raw_dat'])\n",
    "else:\n",
    "    c_graph = create_graph(merged_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57590 nodes in the network.\n",
      "There are 1032557 edges in the network.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {0} nodes in the network.\".format(len(c_graph.nodes()))\n",
    "print \"There are {0} edges in the network.\".format(len(c_graph.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Connected Component\n",
    "\n",
    "The weakly connected component (WCC) is a maximal subgraph of a directed graph such that for every pair of vertices $u$, $v$ in the subgraph, there is an undirected path from $u$ to $v$ and a directed path from $v$ to $u$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wcc(G):\n",
    "    return sorted(nx.weakly_connected_component_subgraphs(G), key=len, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'wcc' in net_dat and net_dat['wcc']:\n",
    "    c_weak =  net_dat['wcc']\n",
    "else:\n",
    "    c_weak = get_wcc(c_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size (number of edges) of subgraph with largest weakly connected component is: 1031478\n"
     ]
    }
   ],
   "source": [
    "print 'The size (number of edges) of subgraph with largest weakly connected component is:', c_weak.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Centrality\n",
    "\n",
    "Historically first and conceptually simplest is degree centrality, which is defined as the number of links incident upon a node (e.g.: the number of ties that a node has). The degree can be interpreted in terms of the immediate risk of a node for catching whatever is flowing through the network (such as a virus, or some information). In the case of a directed network (where ties have direction), usually two separate measures of degree centrality are defined, namely in-degree and out-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deg_cents(G_wcc):\n",
    "    \"\"\"Get degree centrality.\"\"\"\n",
    "    \n",
    "    # in-edges\n",
    "    node_in_degree = nx.in_degree_centrality(G_wcc)\n",
    "    # out-edges\n",
    "    node_out_degree = nx.out_degree_centrality(G_wcc)\n",
    "    return node_in_degree, node_out_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'deg_cent_in' in net_dat and \\\n",
    "    'deg_cent_out' in net_dat and \\\n",
    "    net_dat['deg_cent_in'] and \\\n",
    "    net_dat['deg_cent_out']:\n",
    "        node_in_degree =  net_dat['deg_cent_in']\n",
    "        node_out_degree =  net_dat['deg_cent_out']\n",
    "else:\n",
    "    node_in_degree, node_out_degree = deg_cents(c_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most central companies according to in-edges degree centrality:\n",
      "[(u'Microsoft', 0.028508823049488333),\n",
      " (u'Reuters', 0.027040257348656144),\n",
      " ('Sony', 0.020489988111610992),\n",
      " ('Google', 0.01934777034429707),\n",
      " (u'IBM', 0.01806568917690389),\n",
      " (u'HSBC', 0.017040024242989345),\n",
      " (u'Vodafone', 0.01678360800951071),\n",
      " (u'Apple Inc.', 0.01673698687615096),\n",
      " (u'Nintendo', 0.01631739667591319),\n",
      " (u'Intel', 0.015897806475675423)]\n",
      "\n",
      "10 most central companies according to out-edges degree centrality:\n",
      "[('Telia Company', 0.012331289773654396),\n",
      " ('Hitachi', 0.012214736940255017),\n",
      " (u'Vodafone', 0.012098184106855637),\n",
      " ('Sony', 0.01191169957341663),\n",
      " ('Panasonic', 0.011398867106459357),\n",
      " (u'Toshiba', 0.011375556539779482),\n",
      " (u'Kyocera', 0.011352245973099605),\n",
      " ('Mitsubishi Electric', 0.010862724072822209),\n",
      " ('Comcast', 0.01074617123942283),\n",
      " ('Fujitsu', 0.010722860672742954)]\n"
     ]
    }
   ],
   "source": [
    "print \"10 most central companies according to in-edges degree centrality:\"\n",
    "pprint(Counter(node_in_degree).most_common(10))\n",
    "\n",
    "print \"\\n10 most central companies according to out-edges degree centrality:\"\n",
    "pprint(Counter(node_out_degree).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technology companies have a lead when it comes to most in/out degree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eigen_cents(G_wcc):\n",
    "    \"\"\"Get degree centrality.\"\"\"\n",
    "    \n",
    "    # in-edges\n",
    "    node_in_eigen = nx.eigenvector_centrality(G_wcc)\n",
    "    # out-edges, needs reversed wcc graph\n",
    "    node_out_eigen = nx.eigenvector_centrality(G_wcc.reverse())\n",
    "    return node_in_eigen, node_out_eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'eigen_cent_in' in net_dat and \\\n",
    "    'eigen_cent_out' in net_dat and \\\n",
    "    net_dat['eigen_cent_in'] and \\\n",
    "    net_dat['eigen_cent_out']:\n",
    "        node_in_eigen =  net_dat['eigen_cent_in']\n",
    "        node_out_eigen =  net_dat['eigen_cent_out']\n",
    "else:\n",
    "    node_in_eigen, node_out_eigen = eigen_cents(c_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most central companies according to in-edges eigenvector centrality:\n",
      "[('Whole Foods Market', 0.06513803920756327),\n",
      " ('Costco', 0.06467179872391232),\n",
      " (u'Lidl', 0.0645838642026889),\n",
      " (u'Aldi', 0.06457330500187025),\n",
      " (u'Albertsons', 0.06457029576072434),\n",
      " (u'Ahold Delhaize', 0.06455900783217165),\n",
      " (u'Giant-Carlisle', 0.06450263306524448),\n",
      " (u'Pavilions (supermarket)', 0.06448307712349564),\n",
      " (u'M\\xe1s Club', 0.06445324850334076),\n",
      " (u'Walmart', 0.06444629182605319)]\n",
      "\n",
      "10 most central companies according to out-edges eigenvector centrality:\n",
      "[(u'Albertsons', 0.06500364713613616),\n",
      " (u'Delhaize Group', 0.06497264344789812),\n",
      " (u'Kroger', 0.06490845641815714),\n",
      " (u'SuperValu (United States)', 0.06471331197720417),\n",
      " ('Costco', 0.06469132155925905),\n",
      " ('Whole Foods Market', 0.06468862077172617),\n",
      " (u'Star Market', 0.06437887738082511),\n",
      " (u'Acme Markets', 0.0643407200103748),\n",
      " (u'Aldi', 0.06432459116870955),\n",
      " (u'Stop & Shop', 0.06431557975668435)]\n"
     ]
    }
   ],
   "source": [
    "print \"10 most central companies according to in-edges eigenvector centrality:\"\n",
    "pprint(Counter(node_in_eigen).most_common(10))\n",
    "\n",
    "print \"\\n10 most central companies according to out-edges eigenvector centrality:\"\n",
    "pprint(Counter(node_out_eigen).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the result for eigenvector centrality it was weird that only retail and food companies show up. Once we took a closer look at the article of [Delhaize Group](https://en.wikipedia.org/wiki/Delhaize_Group#See_also) we noted that a lot of external links exists:\n",
    "* Supermarket chains in the United States\n",
    "* Top 20 companies in Belgium (BEL20 companies of Belgium)\n",
    "* etc.\n",
    "\n",
    "Eigenvector centrality is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. Since Delhaize Group has for example Stop & Shop as subsidiary these high eigenvector centrality nodes link to other high eigenvector centrality nodes. \n",
    "\n",
    "Thus the decision was made to exclude the sections:\n",
    "* See also\n",
    "* References \n",
    "* External links\n",
    "\n",
    "They can be useful like in the case of [Microsoft](https://en.wikipedia.org/wiki/Microsoft#External_links) but for the majority these sections are rather misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store network data to file\n",
    "network = {\n",
    "    'graph': c_graph,\n",
    "    'raw_dat': pickle_companies,\n",
    "    'wcc': c_weak,\n",
    "    'deg_cent_in': node_in_degree,\n",
    "    'deg_cent_out': node_out_degree,\n",
    "    'eigen_cent_in': node_in_eigen,\n",
    "    'eigen_cent_out': node_out_eigen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the file for fast reprocessing\n",
    "with open(network_f, 'wb') as f:\n",
    "    pickle.dump(network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free up some memory\n",
    "del network, c_graph, net_dat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Alternative construction\n",
    "\n",
    "The same strategy as above is used to get the data and construct the network from it. All of the calculations were saved in order to execute the final notebook much faster. There will be less links after rebuilding the network because of the skipped sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load network data if available\n",
    "net_dat_red = dict()\n",
    "# load network data if available\n",
    "if os.path.isfile(network_red_f):\n",
    "    with open(network_red_f, 'rb') as f:\n",
    "        net_dat_red = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to rebuild the whole network the wikitext is reparsed while the problematic sections are cut from the string. The current link list will then be intersected with the new list of all links on the page that is cross checked with the list of all links that were returned by the Wikipedia API.\n",
    "\n",
    "This new list of links will also be populated to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'raw_dat' in net_dat_red and net_dat_red['raw_dat']:\n",
    "    merged_companies = net_dat['raw_dat']\n",
    "else:\n",
    "    for c_name, comp in tqdm_notebook(merged_companies.iteritems(), desc='Links'):\n",
    "\n",
    "        # don't add non companies\n",
    "        if 'is_company' not in comp or not comp['is_company']:\n",
    "            continue\n",
    "\n",
    "        # parse wikitext, remove references\n",
    "        wiki_raw_cleaned = re.sub(r'<ref.+?</ref>|<ref>.+?</ref>', '', comp['wiki_raw'])\n",
    "        wiki_code = mwparserfromhell.parse(wiki_raw_cleaned, skip_style_tags=True)\n",
    "        exclude_sections = [u'==See also==', u'==References==', u'==External links==']\n",
    "        # cut the raw wiki text at the found section\n",
    "        for e in exclude_sections:\n",
    "            if e in wiki_code.filter_headings():\n",
    "                # take the first part of the wikicode excluding the unwanted section\n",
    "                wiki_raw_cleaned = wiki_raw_cleaned.split(e)[0]\n",
    "\n",
    "        wiki_code = mwparserfromhell.parse(wiki_raw_cleaned, skip_style_tags=True)\n",
    "        # go through each link in wikitext and extract\n",
    "        links = set()\n",
    "        for link in wiki_code.filter_wikilinks():\n",
    "            # [[Template:Infobox company]]\n",
    "            if re.match(r'\\[\\[.+:.+\\]\\]', unicode(link)):\n",
    "                continue\n",
    "            # [[T\\xe4by]], [[Hertz|MHz]]\n",
    "            matched_wiki_name = re.match(r'\\[\\[(.+?)(?:\\|.+)?\\]\\]', unicode(link))\n",
    "            if matched_wiki_name:\n",
    "                if matched_wiki_name.group(1) in comp['all_links']:\n",
    "                    links.add(matched_wiki_name.group(1))\n",
    "\n",
    "        if c_name == 'Delhaize Group':\n",
    "            print \"Any link in sections without\", exclude_sections, \":\\n\", links, \"\\n\"\n",
    "            print \"All company links, all sections:\\n\", comp['links'], \"\\n\"\n",
    "            comp['links'] = comp['links'].intersection(links)\n",
    "            print \"Final company links without unwanted sections:\\n\", comp['links'], \"\\n\"\n",
    "\n",
    "        # list of links from specific sections intersected with old link list\n",
    "        comp['links'] = comp['links'].intersection(links)\n",
    "        comp['all_links'] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'graph' in net_dat_red and net_dat_red['graph']:\n",
    "    c_graph_reduced =  net_dat_red['graph']\n",
    "else:\n",
    "    c_graph_reduced = create_graph(merged_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56898 nodes in the network.\n",
      "There are 150662 edges in the network.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {0} nodes in the network.\".format(len(c_graph_reduced.nodes()))\n",
    "print \"There are {0} edges in the network.\".format(len(c_graph_reduced.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the size of the number of links decreased dramatically from over a million to ca. 150 thousand. That is only 15% of the original links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Connected Component\n",
    "\n",
    "The first two steps of creating the WCC and calculating some centralities will be like above. However new centrality measures are introduced and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'wcc' in net_dat_red and net_dat_red['wcc']:\n",
    "    c_weak_red =  net_dat_red['wcc']\n",
    "else:\n",
    "    c_weak_red = get_wcc(c_graph_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size (number of edges) of subgraph with largest weakly connected component is: 149251\n"
     ]
    }
   ],
   "source": [
    "print 'The size (number of edges) of subgraph with largest weakly connected component is:', c_weak_red.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'deg_cent_in' in net_dat_red and \\\n",
    "    'deg_cent_out' in net_dat_red and \\\n",
    "    net_dat_red['deg_cent_in'] and \\\n",
    "    net_dat_red['deg_cent_out']:\n",
    "        node_in_degree =  net_dat_red['deg_cent_in']\n",
    "        node_out_degree =  net_dat_red['deg_cent_out']\n",
    "else:\n",
    "    node_in_degree, node_out_degree = deg_cents(c_weak_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most central companies according to in-edges degree centrality:\n",
      "[(u'Microsoft', 0.022857725962396998),\n",
      " (u'IBM', 0.014439143855710605),\n",
      " (u'Google', 0.013265644531748258),\n",
      " (u'Apple Inc.', 0.011556417255542233),\n",
      " (u'Sony', 0.01089313502895482),\n",
      " (u'General Motors', 0.010306385366973647),\n",
      " (u'Intel', 0.009464527156305007),\n",
      " (u'BBC', 0.008954310058930074),\n",
      " (u'General Electric', 0.008903288349192582),\n",
      " (u'Hewlett-Packard', 0.008597158090767621)]\n",
      "\n",
      "10 most central companies according to out-edges degree centrality:\n",
      "[(u'Sony Interactive Entertainment', 0.0025255746320059186),\n",
      " (u'General Motors', 0.0022959769381871984),\n",
      " (u'Electronic Arts', 0.0020918900992372255),\n",
      " (u'The Blackstone Group', 0.0020663792443684787),\n",
      " (u'Participant Media', 0.0020408683894997323),\n",
      " (u'Bain Capital', 0.0020153575346309855),\n",
      " (u'Vodafone', 0.0019898466797622387),\n",
      " (u'Kohlberg Kravis Roberts', 0.0018878032602872522),\n",
      " (u'Sega development studios', 0.0018878032602872522),\n",
      " (u'Dell', 0.001836781550549759)]\n"
     ]
    }
   ],
   "source": [
    "print \"10 most central companies according to in-edges degree centrality:\"\n",
    "pprint(Counter(node_in_degree).most_common(10))\n",
    "\n",
    "print \"\\n10 most central companies according to out-edges degree centrality:\"\n",
    "pprint(Counter(node_out_degree).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example Microsoft, Google and Apple still appear in the top 10 of degree centrality after the approach of the network generation was shifted. Now there is also a greater mix with not only technology companies.\n",
    "\n",
    "First we also had Los Angeles Times and Reuters under the top 10 of degree centrality but that was mainly due to the fact of HTML markup in the reference links, thus it's important to remove it. For example there is a reference in Ford Motor company auoted from the Los Angeles Times.\n",
    "\n",
    "```\n",
    "Following international condemnation of [[apartheid]], Ford divested from South Africa in 1988, and sold its stake in Samcor, although it licensed the use of its brand name to the company.<ref>{{cite news |url=http://articles.latimes.com/1987-06-15/business/fi-4242_1_south-africa |title=Ford Discussing Plans to Divest in South Africa : Firm Would Give 24% Stake to Workers, But Maintain a Presence |first1=Ralph |last1=Vartabedian |first2=Michael |last2=Parks |work=[[Los Angeles Times]] |date=June 15, 1987 |access-date=October 14, 2016}}</ref>\n",
    "```\n",
    "\n",
    "So you can see a lot can be influenced by the way the data is gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'eigen_cent_in' in net_dat_red and \\\n",
    "    'eigen_cent_out' in net_dat_red and \\\n",
    "    net_dat_red['eigen_cent_in'] and \\\n",
    "    net_dat_red['eigen_cent_out']:\n",
    "        node_in_eigen =  net_dat_red['eigen_cent_in']\n",
    "        node_out_eigen =  net_dat_red['eigen_cent_out']\n",
    "else:\n",
    "    node_in_eigen, node_out_eigen = eigen_cents(c_weak_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most central companies according to in-edges eigenvector centrality:\n",
      "[(u'Paramount Pictures', 0.27982289012922407),\n",
      " (u'Warner Bros.', 0.2562183935912465),\n",
      " (u'Metro-Goldwyn-Mayer', 0.2223053205147436),\n",
      " (u'Columbia Pictures', 0.20996826006680364),\n",
      " (u'The Walt Disney Company', 0.20632546831510437),\n",
      " (u'Universal Studios', 0.2029219038821539),\n",
      " (u'20th Century Fox', 0.20199603541820754),\n",
      " (u'Time Warner', 0.16383407029790323),\n",
      " (u'Viacom', 0.15654828077330127),\n",
      " (u'United Artists', 0.14581433181797088)]\n",
      "\n",
      "10 most central companies according to out-edges eigenvector centrality:\n",
      "[(u'Paramount Pictures', 0.1435390426673471),\n",
      " (u'Warner Bros.', 0.13091206710825676),\n",
      " (u'Metro-Goldwyn-Mayer', 0.12999764306368608),\n",
      " (u'HBO', 0.127090340059302),\n",
      " (u'StudioCanal', 0.1200066451120615),\n",
      " (u'Participant Media', 0.1176612428728507),\n",
      " (u'Sony Pictures Worldwide Acquisitions', 0.11629898460142575),\n",
      " (u'PolyGram Filmed Entertainment', 0.11557376690967804),\n",
      " (u'Screen Gems', 0.1147176932869096),\n",
      " (u'MGM Home Entertainment', 0.11168064005834763)]\n"
     ]
    }
   ],
   "source": [
    "print \"10 most central companies according to in-edges eigenvector centrality:\"\n",
    "pprint(Counter(node_in_eigen).most_common(10))\n",
    "\n",
    "print \"\\n10 most central companies according to out-edges eigenvector centrality:\"\n",
    "pprint(Counter(node_out_eigen).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From retail and food stores the eigenvector centrality changed to entertainment companies. \n",
    "\n",
    "TODO: WHY?\n",
    "\n",
    "Furthermore, there is a high number of technology corporations that generally have a high number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links of Paramount Pictures are:\n",
      "[u'First National Pictures', u'Universal Studios', u'List of Paramount executives', u'Viacom Productions', u'CBS Home Entertainment', u'Gulf and Western Industries', u'Gary Sanchez Productions', u'Metro-Goldwyn-Mayer', u'Viacom (original)', u'Walt Disney Studios (division)', u'CBS Television Stations', u'Warner Bros.', u'Nordisk Film', u'Wanda Group', u'Di Bonaventura Pictures', u'Paramount Home Media Distribution', u'NBCUniversal', u'DreamWorks', u'Universal Music Group', u'Famous Players', u'Viacom Media Networks', u'Fake Empire Productions', u'CBS Television Distribution', u'Bad Robot Productions', u'Paramount Stations Group', u'Sony/ATV Music Publishing', u'Capital Cities Communications', u'Skydance Media', u'Cedar Fair', u'DreamWorks Animation', u'Image Entertainment', u'Paramount Famous Productions', u'United International Pictures', u'Republic Pictures', u'Spelling Television', u'Cruise/Wagner Productions', u'Famous Players Film Company', u'Worldvision Enterprises', u'Gaumont Film Company', u'Platinum Dunes', u'Rysher Entertainment', u'Walt Disney Studios Motion Pictures', u'Lucasfilm', u'Paramount Parks', u'Nickelodeon Movies', u'DAMAC Properties', u'Fleischer Studios', u'Paramount Domestic Television', u'Plan B Entertainment', u'MTV Films', u'Paramount Television', u'MCA Inc.', u'Marvel Studios', u'Insurge Pictures', u'Famous Studios', u'Cineplex Odeon Corporation', u'The Walt Disney Company', u'Paramount Animation', u'The Montecito Picture Company', u'Paramount Vantage', u'Columbia Pictures', u'Desilu Productions', u'20th Century Fox', u'Comedy Central Films', u'CBS Television Studios', u'Viacom', u'Chris-Craft Industries', u'National Amusements', u'RKO Pictures']\n"
     ]
    }
   ],
   "source": [
    "node = 'Paramount Pictures'\n",
    "print \"Links of\", node, \"are:\\n\", c_graph_reduced.neighbors(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality\n",
    "\n",
    "Betweenness is a centrality measure of a node within a graph. Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. Compared to eigenvector centrality which accounts for the 'importance' of a node by taking into account the 'importance' of nodes to which it is pointing to (out-edges) or which are pointing at the node (in-edges) it takes a much longer time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-4413a934fd58>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-4413a934fd58>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if 'bet_cent' in net_dat_red and     net_dat_red['bet_cent'] and         node_cent =  net_dat_red['bet_cent']\u001b[0m\n\u001b[0m                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if 'bet_cent' in net_dat_red and \\\n",
    "    net_dat_red['bet_cent']:\n",
    "        node_cent =  net_dat_red['bet_cent']\n",
    "else:\n",
    "    node_cent = nx.betweenness_centrality(c_weak_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most central company according to betweenness centrality:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'node_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9cea47d78728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use to print with betweenness_centrality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Top 10 most central company according to betweenness centrality:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'node_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print \"Top 10 most central company according to betweenness centrality:\"\n",
    "pprint(Counter(node_cent).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness Centrality\n",
    "\n",
    "Closeness centrality is a measure that wasn't introduced in the [course](http://kurser.dtu.dk/course/02805) curriculum but we wanted to know more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the measures below an undirected version of the graph was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'wcc_undir' in net_dat_red and net_dat_red['wcc_undir']:\n",
    "    c_weak_red_undir =  net_dat_red['wcc_undir']\n",
    "else:\n",
    "    # create undirected version of the network\n",
    "    c_weak_red_undir = c_weak_red.to_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity with Louvain algorithm:\n",
      "0.682525868857\n",
      "Number of communities found by Louvain algorithm:\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# compute the best partition\n",
    "partition = community.best_partition(c_weak_red_undir)\n",
    "\n",
    "# report modularity\n",
    "print \"Modularity with Louvain algorithm:\"\n",
    "print community.modularity(partition, c_weak_red_undir)\n",
    "\n",
    "# report number of communities\n",
    "print \"Number of communities found by Louvain algorithm:\"\n",
    "print len(set(partition.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findCommunities(G):\n",
    "    \"\"\"\n",
    "    Partition network with the Infomap algorithm.\n",
    "    Annotates nodes with 'community' id and return number of communities found.\n",
    "    \"\"\"\n",
    "\n",
    "    infomapWrapper = infomap.Infomap(\"--two-level\")\n",
    "    for e in G.edges_iter():\n",
    "        infomapWrapper.addLink(*e)\n",
    "\n",
    "    infomapWrapper.run();\n",
    "    tree = infomapWrapper.tree\n",
    "    communities = {}\n",
    "    for node in tree.leafIter():\n",
    "        communities[node.originalLeafIndex] = node.moduleIndex()\n",
    "\n",
    "    nx.set_node_attributes(G, 'community', communities)\n",
    "    return tree.numTopModules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities found by Infomap algorithm: 2079\n"
     ]
    }
   ],
   "source": [
    "# the infomap function only accepts integers as node names\n",
    "c_weak_red_infomap = nx.convert_node_labels_to_integers(c_weak_red_undir)\n",
    "print \"Number of communities found by Infomap algorithm:\", findCommunities(c_weak_red_infomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store network data to file\n",
    "network_red = {\n",
    "    'graph': c_graph_reduced,\n",
    "    'raw_dat': merged_companies,\n",
    "    'wcc': c_weak_red,\n",
    "    'deg_cent_in': node_in_degree,\n",
    "    'deg_cent_out': node_out_degree,\n",
    "    'eigen_cent_in': node_in_eigen,\n",
    "    'eigen_cent_out': node_out_eigen,\n",
    "    #'bet_cent': node_cent,\n",
    "    'wcc_undir': c_weak_red_undir\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the file for fast reprocessing\n",
    "with open(network_red_f, 'wb') as f:\n",
    "    pickle.dump(network_red, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<type 'datetime.datetime'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-44dfed44ce5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# produces a file that can be imported into the Gephi network visualization tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_gexf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_graph_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgephi_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-357>\u001b[0m in \u001b[0;36mwrite_gexf\u001b[0;34m(G, path, encoding, prettyprint, version)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/networkx/utils/decorators.pyc\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Finally, we call the original function, making sure to close the fobj.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/networkx/readwrite/gexf.pyc\u001b[0m in \u001b[0;36mwrite_gexf\u001b[0;34m(G, path, encoding, prettyprint, version)\u001b[0m\n\u001b[1;32m     75\u001b[0m     writer = GEXFWriter(encoding=encoding,prettyprint=prettyprint,\n\u001b[1;32m     76\u001b[0m                         version=version)\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/networkx/readwrite/gexf.pyc\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, G)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mgraph_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefaultedgetype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/networkx/readwrite/gexf.pyc\u001b[0m in \u001b[0;36madd_nodes\u001b[0;34m(self, G, graph_element)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mnode_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_element\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             node_data=self.add_attributes(\"node\", node_element,\n\u001b[0;32m--> 330\u001b[0;31m                                           node_data, default)\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mnodes_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/networkx/readwrite/gexf.pyc\u001b[0m in \u001b[0;36madd_attributes\u001b[0;34m(self, node_or_edge, xml_obj, data, default)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;31m# static data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'static'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 attr_id = self.get_attr_id(make_str(k), self.xml_type[val_type],\n\u001b[0m\u001b[1;32m    431\u001b[0m                                            node_or_edge, default, mode)\n\u001b[1;32m    432\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attvalue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <type 'datetime.datetime'>"
     ]
    }
   ],
   "source": [
    "# produces a file that can be imported into the Gephi network visualization tool\n",
    "nx.write_gexf(c_graph_reduced, gephi_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What if?\n",
    "\n",
    "What if google disappears and all nodes connecting directly to google with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
