{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We're now switching focus away from the Network Science (for a little bit), beginning to think about _Natural Language Processing_ instead. In other words, today will be all about teaching your computer to \"understand\" text. This ties in nicely with our work on wikipedia, since wikipedia is a network of connected pieces of text. We've looked at the network so far - now, let's see if we can include the text. Today is about \n",
    "\n",
    "* Installing the _natural language toolkit_ (NLTK) package and learning the basics of how it works (Chapter 1)\n",
    "* Figuring out how to make NLTK to work with other types of text (Chapter 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and the basics\n",
    "\n",
    "> _Reading_\n",
    "> The reading for today is Natural Language Processing with Python, first edition (NLPP1e) Chapter 1, Sections 1.1, 1.2, 1.3\\. [It's free online](http://www.nltk.org/book_1ed/). \n",
    "> \n",
    "> * **Important**: Do not use the newest version of this book. Use the first edition. (The newest version is based on on Python 3).\n",
    "> * **Important**: Seriously, remember that we're using the *first edition*.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercises_: NLPP1e Chapter 1\\.\n",
    "> \n",
    "> * First, install `nltk` if it isn't installed already (there are some tips below that I recommend checking out before doing installing)\n",
    "> * Second, work through chapter 1. The book is set up as a kind of tutorial with lots of examples for you to work through. I recommend you read the text with an open IPython Notebook and type out the examples that you see. ***It becomes much more fun if you to add a few variations and see what happens***. Some of those examples might very well be due as assignments (see below the install tips), so those ones should definitely be in a `notebook`. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "#text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\1st Semester Master\\Social Graphs and interaction\\lesson6\\nltk\n",
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n",
      "None \n",
      "\n",
      "imperial subtly impalpable pitiable curious abundant perilous\n",
      "trustworthy untoward singular lamentable few determined maddens\n",
      "horrible tyrannical lazy mystifying christian exasperate\n",
      "None \n",
      "\n",
      "a_pretty is_pretty a_lucky am_glad be_glad\n",
      "None\n",
      "<module 'nltk.book' from 'D:\\Applications\\anaconda2\\lib\\site-packages\\nltk\\book.pyc'>\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "import os\n",
    "import pprint\n",
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "ntlk_data=os.getcwd() + \"\\\\nltk\"  #data resource\n",
    "nltk.data.path.append(ntlk_data)\n",
    "print (ntlk_data)\n",
    "from nltk.book import *\n",
    "print (text1.concordance(\"monstrous\"),\"\\n\")\n",
    "print (text1.similar(\"monstrous\"), \"\\n\")\n",
    "print (text2.common_contexts([\"monstrous\", \"very\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Install tips \n",
    "\n",
    "Check to see if `nltk` is installed on your system by typing `import nltk` in a `notebook`. If it's not already installed, install it as part of _Anaconda_ by typing \n",
    "\n",
    "     conda install nltk \n",
    "\n",
    "at the command prompt. If you don't have them, you can download the various corpora using a command-line version of the downloader that runs in Python notebooks: In the iPython notebook, run the code \n",
    "\n",
    "     import nltk\n",
    "     nltk.download()\n",
    "\n",
    "Now you can hit `d` to download, then type \"book\" to fetch the collection needed today's `nltk` session. Now that everything is up and running, let's get to the actual exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercises_: NLPP1e Chapter 1 (the stuff that might be due in an upcoming assignment).\n",
    "> \n",
    "> The following exercises from Chapter 1 are what might be due for an assignment later one.\n",
    ">\n",
    "> * Try out the `concordance` method, using another text and a word of your own choosing.\n",
    "> * Also try out the `similar` and `common_context` methods for a few of your own examples.\n",
    "> * Create your own version of a dispersion plot (\"your own version\" means another text and different word).\n",
    "> * Explain in your own words what aspect of language _lexical diversity_ describes. \n",
    "> * Create frequency distributions for `text2`, including the cumulative frequency plot for the 75 most common words.\n",
    "> * What is a bigram? How does it relate to `collocations`. Explain in your own words.\n",
    "> * Work through ex 2-12 in NLPP's section 1.8\\. \n",
    "> * Work through exercise 15, 17, 19, 22, 23, 26, 27, 28 in section 1.8\\. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4\n",
      "3142930641582938830174357788501626427282669988762475256374173175398995908420104023465432599069702289330964075081611719197835869803511992549376\n",
      "['Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python', 'Monty', 'Python']\n",
      "text1 length 260819 distince words 19317 \n",
      "\n",
      "Lexical diversity humor 4.32429738888 Lexical diversity romance 8.28466635116 \n",
      "\n",
      "wanna chat; PART JOIN; MODE #14-19teens; JOIN PART; PART PART;\n",
      "cute.-ass MP3; MP3 player; JOIN JOIN; times .. .; ACTION watches; guys\n",
      "wanna; song lasts; last night; ACTION sits; -...)...- S.M.R.; Lime\n",
      "Player; Player 12%; dont know; lez gurls; long time\n",
      "None \n",
      "\n",
      "[u'b', u'b-day', u'b/c', u'b4', u'babay', u'babble', u'babblein', u'babe', u'babes', u'babi', u'babies', u'babiess', u'baby', u'babycakeses', u'bachelorette', u'back', u'backatchya', u'backfrontsidewaysandallaroundtheworld', u'backroom', u'backup', u'bacl', u'bad', u'bag', u'bagel', u'bagels', u'bahahahaa', u'bak', u'baked', u'balad', u'balance', u'balck', u'ball', u'ballin', u'balls', u'ban', u'band', u'bandito', u'bandsaw', u'banjoes', u'banned', u'baord', u'bar', u'barbie', u'bare', u'barely', u'bares', u'barfights', u'barks', u'barn', u'barrel', u'base', u'bases', u'basically', u'basket', u'battery', u'bay', u'bbbbbyyyyyyyeeeeeeeee', u'bbiam', u'bbl', u'bbs', u'bc', u'be', u'beach', u'beachhhh', u'beam', u'beams', u'beanbag', u'beans', u'bear', u'bears', u'beat', u'beaten', u'beatles', u'beats', u'beattles', u'beautiful', u'because', u'beckley', u'become', u'bed', u'bedford', u'bedroom', u'beeeeehave', u'beeehave', u'been', u'beer', u'before', u'beg', u'begin', u'behave', u'behind', u'bein', u'being', u'beleive', u'believe', u'belive', u'bell', u'belly', u'belong', u'belongings', u'ben', u'bend', u'benz', u'bes', u'beside', u'besides', u'best', u'bet', u'betrayal', u'betta', u'better', u'between', u'beuty', u'bf', u'bi', u'biatch', u'bible', u'biebsa', u'bied', u'big', u'bigest', u'biggest', u'biiiatch', u'bike', u'bikes', u'bikini', u'bio', u'bird', u'birfday', u'birthday', u'bisexual', u'bishes', u'bit', u'bitch', u'bitches', u'bitdh', u'bite', u'bites', u'biyatch', u'biz', u'bj', u'black', u'blade', u'blah', u'blank', u'blankie', u'blazed', u'bleach', u'blech', u'bless', u'blessings', u'blew', u'blind', u'blinks', u'bliss', u'blocking', u'bloe', u'blood', u'blooded', u'bloody', u'blow', u'blowing', u'blowjob', u'blowup', u'blue', u'blueberry', u'bluer', u'blues', u'blunt', u'board', u'bob', u'bodies', u'body', u'boed', u'boght', u'boi', u'boing', u'boinked', u'bois', u'bomb', u'bone', u'boned', u'bones', u'bong', u'boning', u'bonus', u'boo', u'booboo', u'boobs', u'book', u'boom', u'boooooooooooglyyyyyy', u'boost', u'boot', u'bootay', u'booted', u'boots', u'booty', u'border', u'borderline', u'bored', u'boredom', u'boring', u'born', u'born-again', u'bosom', u'boss', u'bossy', u'bot', u'both', u'bother', u'bothering', u'bottle', u'bought', u'bounced', u'bouncer', u'bouncers', u'bound', u'bout', u'bouts', u'bow', u'bowl', u'box', u'boy', u'boyfriend', u'boys', u'bra', u'brad', u'brady', u'brain', u'brakes', u'brass', u'brat', u'brb', u'brbbb', u'bread', u'break', u'breaks', u'breath', u'breathe', u'bred', u'breeding', u'bright', u'brightened', u'bring', u'brings', u'bro', u'broke', u'brooklyn', u'brother', u'brothers', u'brought', u'brown', u'brrrrrrr', u'bruises', u'brunswick', u'brwn', u'btw', u'bucks', u'buddyyyyyy', u'buff', u'buffalo', u'bug', u'bugs', u'buh', u'build', u'builds', u'built', u'bull', u'bulls', u'bum', u'bumber', u'bummer', u'bumped', u'bumper', u'bunch', u'bunny', u'burger', u'burito', u'burned', u'burns', u'burp', u'burpin', u'burps', u'burried', u'burryed', u'bus', u'buses', u'bust', u'busted', u'busy', u'but', u'butt', u'butter', u'butterscotch', u'button', u'buttons', u'buy', u'buying', u'bwahahahahahahahahahaha', u'by', u'byb', u'bye', u'byeee', u'byeeee', u'byeeeeeeee', u'byeeeeeeeeeeeee', u'byes'] \n",
      "\n",
      "index of sunset 629\n",
      "[u'!!!!', u'!!!.', u'!...', u'!???', u'\"...', u'####', u'((((', u'))))', u',,,,', u'.. .', u'....', u'.op.', u'1.98', u'1.99', u'100%', u'10th', u'1200', u'1299', u'18ST', u'1900', u'1930', u'1980', u'1985', u'1996', u'1cos', u'2006', u'2:55', u'2DAY', u'2Pac', u'39.3', u'3:45', u'4.20', u'45.5', u'4:03', u'64.8', u'6:38', u'6:41', u'6:51', u'6:53', u'7:45', u'9.53', u'98.5', u'98.6', u'9:10', u':o *', u'; ..', u'<---', u\"<3's\", u'<333', u'<<<<', u'<~~~', u'>:->', u'?!?!', u'??!!', u'????', u'AKDT', u'AKST', u'AWAY', u'Ahhh', u'Away', u'Awww', u'Back', u'Been', u'Bone', u'Born', u'Boyz', u'CALI', u'CAPS', u'CHAT', u'COME', u'Came', u'Care', u'Chat', u'Chop', u'City', u'Come', u'Cool', u'Ctrl', u'Cute', u'DAMN', u'DING', u'DOES', u'DONT', u'Damn', u'Dang', u'Dawn', u'Days', u'Deep', u'Does', u'Dood', u'Down', u'Drew', u'Drop', u'Dude', u'ELSE', u'EVEN', u'Eggs', u'Elev', u'Elle', u'Even', u'Evil', u'Eyes', u'FACE', u'FINE', u'FROM', u'Fade', u'Food', u'Fort', u'From', u'GIRL', u'GOOD', u'GUYS', u'Girl', u'Good', u'Gosh', u'GrlZ', u'HAHA', u'HALO', u'HAVE', u'HERE', u'HOTT', u'HUGE', u'Haha', u'Hail', u'Hand', u'Hard', u'Have', u'Help', u'Here', u'Hero', u'Heya', u'Heys', u'Heyy', u'High', u'Hill', u'Hiya', u'Hold', u'Holy', u'Home', u'Hott', u'Hugs', u'Iowa', u'JOIN', u'JUST', u'Jane', u'Jess', u'Joey', u'John', u'Judy', u'Just', u'KNOW', u'Kent', u'Kewl', u'Kick', u'Kids', u'King', u'Kiss', u'KoOL', u'Kold', u'LATE', u'LAst', u'LIVE', u'LMAO', u'LONG', u'LOUD', u'Last', u'Lets', u'Liam', u'Lies', u'Life', u'Like', u'Lime', u'Lion', u'Live', u'Lmao', u'LoVe', u'Long', u'Look', u'Lord', u'Love', u'MODE', u'MORE', u'MRIs', u'MUAH', u'Male', u'Maps', u'Mary', u'Matt', u'Meep', u'Mine', u'Mono', u'NAME', u'NICK', u'NONE', u'NTMN', u'Need', u'News', u'Nice', u'None', u'Nooo', u'Nope', u'Nova', u'O.k.', u'OOPS', u'Ohhh', u'Ohio', u'Okay', u'Only', u'Oops', u'Over', u'PART', u\"PM's\", u'PMSL', u'Paul', u'Phil', u'Poor', u'Pour', u'Prof', u'QUIT', u\"RN's\", u'ROFL', u'ROOM', u'Rang', u'Reub', u'Rick', u'Road', u'Rock', u'Rofl', u'Room', u'Rule', u'Rush', u'Ruth', u'SEEN', u'SExy', u'SIZE', u'SOME', u'SSRI', u'STOP', u'Same', u'Sat.', u'Save', u'Seee', u'Sexy', u'Show', u'Slip', u'Song', u'Stop', u'Sure', u'Swim', u'TALK', u'TEXT', u'THAT', u'THEY', u'TIME', u'TYPR', u'Take', u'Talk', u'Teck', u'Tell', u'That', u'Then', u'They', u'This', u'Tide', u'Tiff', u'Time', u'Tina', u'Tisk', u'Troy', u'Turn', u'Type', u'U100', u'U101', u'U102', u'U103', u'U104', u'U105', u'U106', u'U107', u'U108', u'U109', u'U110', u'U111', u'U112', u'U113', u'U114', u'U115', u'U116', u'U117', u'U118', u'U119', u'U120', u'U121', u'U122', u'U123', u'U126', u'U128', u'U129', u'U130', u'U132', u'U133', u'U134', u'U136', u'U137', u'U138', u'U139', u'U141', u'U142', u'U143', u'U144', u'U145', u'U146', u'U147', u'U148', u'U149', u'U150', u'U153', u'U154', u'U155', u'U156', u'U158', u'U163', u'U164', u'U165', u'U168', u'U169', u'U170', u'U172', u'U175', u'U181', u'U190', u'U196', u'U197', u'U219', u'U520', u'U542', u'U819', u'U820', u'U988', u'U989', u'Uhhh', u'Ummm', u'VBox', u'VVil', u'Very', u'WHEN', u'WHOA', u'WILL', u'WITH', u'Well', u'Werd', u'Were', u'West', u'What', u'When', u'Will', u'Wind', u'Wyte', u'YALL', u'YOUR', u'Yeah', u'Yoko', u'York', u'Your', u'able', u'abou', u'acid', u'adds', u'addy', u'ages', u'ahah', u'ahem', u'ahhh', u'aime', u'aint', u'akon', u'allo', u'ally', u'alot', u'also', u'amen', u'anal', u'anti', u'any1', u'area', u'argh', u'arms', u'army', u'asks', u'asss', u'aunt', u'away', u'awww', u'babe', u'babi', u'baby', u'back', u'bacl', u'ball', u'band', u'bare', u'barn', u'base', u'beam', u'bear', u'beat', u'been', u'beer', u'bein', u'bell', u'bend', u'benz', u'best', u'bied', u'bike', u'bird', u'bite', u'blah', u'blew', u'bloe', u'blow', u'blue', u'body', u'boed', u'bois', u'bomb', u'bone', u'bong', u'book', u'boom', u'boot', u'born', u'boss', u'both', u'bout', u'bowl', u'boys', u'brad', u'brat', u'bred', u'brwn', u'buff', u'bugs', u'bull', u'burp', u'bust', u'busy', u'butt', u'byes', u'caan', u'caca', u'cali', u'call', u'calm', u'came', u'cams', u'cant', u'caps', u'card', u'care', u'cars', u'case', u'cash', u'cast', u'cell', u'cepn', u'chat', u'chik', u'chip', u'chit', u'choc', u'ciao', u'city', u'clap', u'clay', u'club', u'clue', u'cmon', u'coat', u'cock', u'coem', u'cold', u'come', u'comp', u'cook', u'cool', u'cops', u'corn', u'cost', u'crap', u'crib', u'crop', u'cums', u'cure', u'cuss', u'cute', u'cyas', u'daft', u'damn', u'dang', u'dark', u'date', u'dawg', u'days', u'dead', u'deaf', u'deal', u'dear', u'deep', u'deop', u'dick', u'died', u'dies', u'dint', u'dirt', u'disc', u'dman', u'docs', u'does', u'dogs', u'doin', u'dojn', u'doll', u'done', u'dont', u'door', u'dork', u'dotn', u'down', u'draw', u'drew', u'drop', u'drug', u'dude', u'duet', u'dumb', u'dump', u'dust', u'dyed', u'each', u'ears', u'east', u'easy', u'eats', u'ebay', u'eeek', u'eeww', u'elle', u'ello', u'else', u'enuf', u'eric', u'este', u'evah', u'even', u'ever', u'evil', u'ewww', u\"ex's\", u'exit', u'eyes', u'face', u'fair', u'fake', u'fall', u'fart', u'fast', u'fawk', u'fear', u'feat', u'febe', u'feel', u'feet', u'felt', u'find', u'fine', u'fire', u'firs', u'fish', u'fits', u'five', u'flaw', u'flow', u'fock', u'food', u'fool', u'foot', u'form', u'four', u'free', u'from', u'frst', u'fuck', u'full', u'gags', u'gals', u'game', u'gawd', u'gays', u'gear', u'gees', u'geez', u'gets', u'ghet', u'gift', u'gimp', u'girl', u'giva', u'give', u'givs', u'glad', u'goes', u'goin', u'gold', u'golf', u'gone', u'good', u'goof', u'gooo', u'gosh', u'gray', u'grea', u'gret', u'grew', u'grin', u'grrl', u'grrr', u'guns', u'guts', u'guys', u'guyz', u'haaa', u'haha', u'hail', u'hair', u'half', u'hall', u'halo', u'hand', u'hang', u'hank', u'hard', u'hate', u'have', u'hawT', u'hawt', u'haze', u'hazy', u'head', u'heal', u'hear', u'heat', u'heck', u'heee', u'hehe', u'hell', u'help', u'herE', u'herd', u'here', u'heya', u'heyy', u'hgey', u'hick', u'hide', u'high', u'hiii', u'hill', u'hint', u'hiom', u'hits', u'hiya', u'hmmm', u'hmph', u'hogs', u'hola', u'hold', u'holy', u'home', u'hong', u'hook', u'hooo', u'hope', u'hots', u'hott', u'hour', u'howl', u'hows', u'howz', u'http', u'huge', u'hugs', u'humm', u'hump', u'hurr', u'hurt', u'icky', u'idea', u'idnt', u'imma', u'inch', u'into', u'isnt', u\"it's\", u'itch', u'jack', u'jail', u'jeep', u'jeff', u'jerk', u'john', u'joke', u'jude', u'jump', u'junk', u'jush', u'just', u'keep', u'kent', u'kept', u'kewl', u'keys', u'kick', u'kids', u'kill', u'kina', u'kind', u'king', u'kiss', u'kmph', u'knee', u'knew', u'know', u'kold', u'kong', u'kool', u'lady', u'ladz', u'laid', u'lake', u'lala', u'lame', u'land', u'lapd', u'last', u'late', u'lawl', u'lazy', u'lead', u'left', u'legs', u'lets', u'lick', u'lies', u'life', u'like', u'limp', u'line', u'lisa', u'list', u'live', u'lmao', u'lois', u'lol.', u'long', u'look', u'lool', u'lord', u'lose', u'loss', u'lost', u'lots', u'loud', u'love', u'ltnc', u'ltns', u'lube', u'luck', u'lung', u'lust', u'luvs', u'lyin', u'made', u'mahn', u'main', u'make', u'male', u'mama', u'mame', u'mami', u'mang', u'many', u'mark', u'mary', u'mass', u'mauh', u'mean', u'meat', u'meds', u'meet', u'mena', u'menu', u'mess', u'mike', u'mind', u'mine', u'mins', u'miss', u'mite', u'mkay', u'mmmm', u'mode', u'mofo', u'moms', u'mono', u'moon', u'more', u'most', u'move', u'much', u'must', u'n9ne', u'nada', u'nads', u'name', u'nana', u'nawp', u'nawt', u'near', u'neck', u'need', u'nerd', u'newp', u'next', u'nice', u'nick', u'nite', u'nods', u'none', u'nope', u'nose', u'note', u'noth', u'nude', u'nuff', u'numb', u'o.k.', u'offa', u'ogan', u'ohhh', u'ohio', u'ohwa', u\"ok'd\", u'okay', u'okey', u'once', u'ones', u'only', u'ooer', u'oooh', u'oops', u'open', u'opps', u'orgy', u'orta', u'otay', u'ouch', u'out.', u'outa', u'outs', u'over', u'owww', u'page', u'paid', u'pain', u'pair', u'park', u'part', u'pasa', u'pass', u'past', u'peek', u'peel', u'perk', u'perv', u'pfft', u'phil', u'pick', u'pics', u'pies', u'piff', u'pigs', u'pimp', u'pine', u'pink', u'plan', u'play', u'plow', u'plus', u\"pm'n\", u\"pm's\", u'pmsl', u'poem', u'poll', u'poof', u'pool', u'poop', u'poor', u'poot', u'pope', u'pork', u'porn', u'post', u'pour', u'pray', u'prep', u'prob', u'puff', u'puke', u'pull', u'pure', u'push', u'puts', u'pwns', u'ques', u'quit', u'quiz', u'raed', u'rain', u'rang', u'rape', u'rats', u'read', u'real', u'rent', u'rest', u'ribs', u'rich', u'ride', u'ring', u'road', u'rock', u'rofl', u'roll', u'roof', u'room', u'root', u'rose', u'rubs', u'ruff', u'rule', u'runs', u'rush', u'saME', u'safe', u'said', u'salt', u'same', u'samn', u'sand', u'sang', u'sayn', u'says', u'scar', u'scuk', u'scum', u'sean', u'seat', u'seee', u'seem', u'seen', u'self', u'sell', u'send', u'sent', u'serg', u'seth', u'sets', u'sexi', u'sexs', u'sext', u'sexy', u'shes', u'shit', u'shop', u'shot', u'show', u'shup', u'shut', u'sick', u'side', u'sigh', u'sign', u'sing', u'sink', u'sips', u'site', u'sits', u'size', u'skin', u'slam', u'slap', u'slip', u'slow', u'smax', u'snow', u'sock', u'soda', u'soft', u'some', u'song', u'soon', u'sooo', u'sore', u'sori', u'sort', u'soul', u'soup', u'span', u'spat', u'spin', u'spit', u'spot', u'ssid', u'star', u'stay', u'stop', u'such', u'suck', u'sum1', u'sure', u'surf', u'swim', u'syck', u't he', u'tail', u'take', u'talk', u'tall', u'tape', u'tart', u'team', u'teck', u'tell', u'temp', u'tend', u'tenn', u'tere', u'term', u'test', u'thah', u'than', u'that', u'them', u'then', u'ther', u'they', u'this', u'thje', u'thnx', u'thot', u'thru', u'tick', u'tiff', u'till', u'time', u'tips', u'tisk', u'tits', u'tjhe', u'tlak', u'tock', u'toes', u'toke', u'told', u'took', u'tooo', u'toop', u'tory', u'toss', u'town', u'trip', u'true', u'tthe', u'tune', u'turn', u'twin', u'twit', u'type', u'typo', u'tyvm', u'ugly', u'ummm', u'urls', u'used', u'uses', u'ussy', u'uyes', u'vamp', u'vega', u'vent', u'very', u'vote', u'wOOt', u'waaa', u'wack', u'waht', u'wait', u'walk', u'wall', u'wana', u'want', u'warm', u'wash', u'wats', u'ways', u'wazz', u'wean', u'wear', u'weed', u'week', u'well', u'went', u'were', u'west', u'what', u'when', u'wher', u'whew', u'whip', u'whoa', u'whoo', u'whos', u'whou', u'whud', u'whys', u'wide', u'wife', u'wild', u'will', u'wind', u'wine', u'wins', u'wire', u'wish', u'with', u'woah', u'wont', u'wood', u'woof', u'wooo', u'woot', u'word', u'wore', u'work', u'worl', u'wrap', u'wrek', u'wubs', u'wuts', u'xbox', u'xmas', u'yada', u'yall', u'yard', u'yawn', u'yeah', u'year', u'yeas', u'yeee', u'yell', u'yes.', u'yesh', u'yess', u'yoko', u'yoll', u'your', u'yout', u\"yw's\", u'z-ro', u'zone']\n",
      "\n",
      "Uppercase: [u'PATSY', u'PERSON', u'MAN', u'GUARD', u'OF', u'HERBERT', u'CHARACTERS', u'A', u'HEAD', u'ROGER', u'MINSTREL', u'HEADS', u'PRINCE', u'W', u'B', u'C', u'MONKS', u'Y', u'OFFICER', u'BRIDE', u'LAUNCELOT', u'SENTRY', u'SUN', u'LEFT', u'CHARACTER', u'CAMERAMAN', u'RANDOM', u'DINGO', u'ZOOT', u'ROBIN', u'CART', u'CONCORDE', u'GOD', u'HISTORIAN', u'CARTOON', u'CRASH', u'PRINCESS', u'DENNIS', u'ENCHANTER', u'SECOND', u'GIRLS', u'I', u'BRIDGEKEEPER', u'FRENCH', u'MASTER', u'KNIGHT', u'SIR', u'VOICE', u'CUSTOMER', u'WOMAN', u'CROWD', u'VILLAGERS', u'AMAZING', u'OTHER', u'NARRATOR', u'ARTHUR', u'DIRECTOR', u'PRISONER', u'BORS', u'VILLAGER', u'GUEST', u'BROTHER', u'GALAHAD', u'ARMY', u'LOVELY', u'PARTY', u'LUCKY', u'O', u'CRAPPER', u'STUNNER', u'CRONE', u'WITCH', u'MIDDLE', u'WINSTON', u'ALL', u'GREEN', u'MAYNARD', u'MIDGET', u'PIGLET', u'NI', u'WIFE', u'KNIGHTS', u'BLACK', u'RIGHT', u'ANIMATOR', u'FATHER', u'THE', u'S', u'OLD', u'GUARDS', u'INSPECTOR', u'GUESTS', u'DEAD', u'TIM', u'SOLDIER', u'KING', u'SHRUBBER', u'SCENE', u'BEDEVERE', u'U', u'N']\n",
      "\n",
      "average word length 3.83041112802\n"
     ]
    }
   ],
   "source": [
    "##bigram is a word pair we generate bigram from each two adjacnet words from a text and look for the frequent pairs, aka. collocation\n",
    "##############exercises\n",
    "print (12 / (4 + 1))\n",
    "\n",
    "print (26 ** 100)\n",
    "\n",
    "print (['Monty', 'Python'] * 20)\n",
    "\n",
    "from nltk.book import *\n",
    "print (\"text1 length\",len(text1),\"distince words\",len(set(text1)), \"\\n\")\n",
    "\n",
    "from nltk.corpus import brown\n",
    "#print (\"brown categories\",brown.categories())\n",
    "print (\"Lexical diversity humor\",len(brown.words(categories=\"humor\"))/len(set(brown.words(categories=\"humor\"))), \"Lexical diversity romance\",len(brown.words(categories=\"romance\"))/len(set(brown.words(categories=\"romance\"))),\"\\n\")\n",
    "\n",
    "#Sense and Sensibility\n",
    "#text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\"])\n",
    "\n",
    "print (text5.collocations(),'\\n')\n",
    "\n",
    "#####exercise 15\n",
    "print (sorted([w for w in set(text5) if w.startswith('b')]),\"\\n\")\n",
    "\n",
    "#####exercise 17 find the slice for the complete sentence that contains this word\n",
    "print (\"index of sunset\",text9.index(\"sunset\"))\n",
    "\n",
    "#for sent in sent_tokenize_list:\n",
    "#    if \"sunset\" in set(sent):\n",
    "#        print (\"the sentence is :\",sent)\n",
    "#        break\n",
    "        \n",
    "######exercise 19 What is the difference between the following two lines? Which one will give a larger value? Will this be the case for other texts?sorted(set([w.lower() for w in text1]))\n",
    "# \"ex19\",sorted(set([w.lower() for w in text1]))) sort both dinstincts words and indeices\n",
    "# \"ex19\",sorted([w.lower() for w in set(text1)])) sort distinct words\n",
    " \n",
    "######exercise 22 Find all the four-letter words in the Chat Corpus (text5). With the help of a frequency distribution (FreqDist), show these words in decreasing order of frequency.\n",
    "fdist5=FreqDist(text5)\n",
    "print (sorted([w for w in set(text5) if len(w) == 4]))\n",
    "\n",
    "######exercise 23 Use a combination of for and if statements to loop over the words of the movie script for Monty Python and the Holy Grail (text6) and print all the uppercase words, one per line.\n",
    "uppers=[]\n",
    "for w in set(text6):\n",
    "    if w.isupper():\n",
    "        uppers.append(w)\n",
    "print(\"\\nUppercase:\",uppers) \n",
    "\n",
    "######exercise 26 What does the following Python code do? sum([len(w) for w in text1]) Can you use it to work out the average word length of a text?\n",
    "print (\"\\naverage word length\",sum([len(w) for w in text1]) / len(text1))\n",
    "\n",
    "######exercise 27 Define a function called vocab_size(text) that has a single parameter for the text, and which returns the vocabulary size of the text\n",
    "def vocab_size():\n",
    "    \n",
    "\n",
    "######exercise 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with NLTK and other types of text\n",
    "\n",
    "So far, we've worked with text from Wikipedia. But that's not the only source of text in the universe. In fact, it's far from it. Chapter 2 in NLPP1e is all about getting access to nicely curated texts that you can find built into NLTK. \n",
    "> \n",
    "> _Reading_: NLPP1e Chapter 2.1 - 2.4\\.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercises_: NLPP1e Chapter 2\\.\n",
    "> \n",
    "> * Solve exercise 4, 8, 11, 15, 16, 17, 18 in NLPP1e, section 2.8\\. As always, I recommend you write up your solutions nicely in a `notebook`.\n",
    "> * Work through exercise 2.8.23 on Zipf's law. [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) connects to a property of the Barabasi-Albert networks. Which one? Take a look at [this article](http://www.hpl.hp.com/research/idl/papers/ranking/adamicglottometrics.pdf) and write a paragraph or two describing other important instances of power-laws found on the internet.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise The amount of the files in state_union 65\n",
      "women: 2 men: 2 people: 10\n",
      "women: 7 men: 12 people: 49\n",
      "women: 2 men: 7 people: 12\n",
      "women: 1 men: 5 people: 22\n",
      "women: 1 men: 2 people: 15\n",
      "women: 2 men: 6 people: 15\n",
      "women: 2 men: 8 people: 10\n",
      "women: 0 men: 3 people: 17\n",
      "women: 0 men: 2 people: 15\n",
      "women: 0 men: 4 people: 26\n",
      "women: 2 men: 2 people: 30\n",
      "women: 2 men: 5 people: 11\n",
      "women: 1 men: 2 people: 19\n",
      "women: 1 men: 4 people: 11\n",
      "women: 0 men: 2 people: 10\n",
      "women: 0 men: 6 people: 10\n",
      "women: 2 men: 6 people: 10\n",
      "women: 0 men: 0 people: 3\n",
      "women: 5 men: 8 people: 12\n",
      "women: 1 men: 3 people: 3\n",
      "women: 0 men: 7 people: 16\n",
      "women: 3 men: 12 people: 14\n",
      "women: 1 men: 12 people: 35\n",
      "women: 1 men: 11 people: 25\n",
      "women: 0 men: 4 people: 17\n",
      "women: 2 men: 5 people: 6\n",
      "women: 0 men: 2 people: 23\n",
      "women: 0 men: 1 people: 32\n",
      "women: 0 men: 1 people: 7\n",
      "women: 0 men: 1 people: 9\n",
      "women: 0 men: 0 people: 20\n",
      "women: 0 men: 0 people: 14\n",
      "women: 1 men: 3 people: 18\n",
      "women: 1 men: 2 people: 19\n",
      "women: 1 men: 0 people: 26\n",
      "women: 1 men: 0 people: 15\n",
      "women: 2 men: 1 people: 12\n",
      "women: 1 men: 1 people: 11\n",
      "women: 2 men: 1 people: 17\n",
      "women: 7 men: 3 people: 19\n",
      "women: 5 men: 3 people: 27\n",
      "women: 1 men: 1 people: 12\n",
      "women: 2 men: 2 people: 14\n",
      "women: 0 men: 1 people: 24\n",
      "women: 0 men: 1 people: 17\n",
      "women: 3 men: 2 people: 13\n",
      "women: 2 men: 3 people: 9\n",
      "women: 2 men: 2 people: 14\n",
      "women: 7 men: 7 people: 13\n",
      "women: 4 men: 4 people: 27\n",
      "women: 2 men: 1 people: 45\n",
      "women: 1 men: 1 people: 66\n",
      "women: 3 men: 1 people: 73\n",
      "women: 3 men: 2 people: 43\n",
      "women: 2 men: 1 people: 31\n",
      "women: 2 men: 2 people: 22\n",
      "women: 3 men: 2 people: 22\n",
      "women: 7 men: 5 people: 41\n",
      "women: 3 men: 3 people: 15\n",
      "women: 3 men: 1 people: 12\n",
      "women: 6 men: 3 people: 14\n",
      "women: 4 men: 6 people: 33\n",
      "women: 8 men: 7 people: 21\n",
      "women: 11 men: 8 people: 18\n",
      "women: 7 men: 7 people: 22\n",
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "####exercise 4\n",
    "from nltk.corpus import state_union \n",
    "from collections import Counter\n",
    "file_list=state_union.fileids()\n",
    "print (\"Exercise The amount of the files in state_union\",len(file_list))\n",
    "#approach 1 \n",
    "#for state_file in file_list:\n",
    "#    wordcounts = Counter(w.lower() for w in state_union.words(state_file))\n",
    "#    women=wordcounts[\"women\"]\n",
    "#    men=wordcounts[\"men\"]\n",
    "#    people=wordcounts[\"people\"]\n",
    "#    print (women,men,people,state_file)\n",
    "#approach 2\n",
    "for state_file in file_list:\n",
    "    fdist = nltk.FreqDist([w.lower() for w in state_union.words(state_file)])\n",
    "    print(\"women:\",fdist[\"women\"],\"men:\",fdist[\"men\"],\"people:\",fdist[\"people\"])\n",
    "\n",
    "#use conditional frequency distribution to plot the trend of three words along the texts\n",
    "cfd_2=nltk.ConditionalFreqDist((target,fileid) \n",
    "                              for fileid in state_union.fileids() \n",
    "                              for w in state_union.words(fileid)\n",
    "                              for target in ['women','men','people']\n",
    "                              if w.lower() in target)\n",
    "#cfd_2.plot()\n",
    "\n",
    "\n",
    "######exercise 8 Define a conditional frequency distribution over the Names corpus that allows you to see which initial letters are more frequent for males vs. females\n",
    "from nltk.corpus import names \n",
    "name_list=names.fileids()\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "cfd_8 = nltk.ConditionalFreqDist(\n",
    "           (fileid, name[0])\n",
    "          for fileid in names.fileids()\n",
    "          for name in names.words(fileid))\n",
    "cfd_8.plot()\n",
    "\n",
    "##exercise 11 Investigate the table of modal distributions and look for other patterns. \n",
    "###Try to explain them in terms of your own impressionistic understanding of the different genres. \n",
    "###Can you find other closed classes of words that exhibit significant differences across different genres?\n",
    "\n",
    "cfd_11 = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd_11.tabulate(conditions=genres, samples=modals)\n",
    "##exercise 15\n",
    "##exercise 16\n",
    "##exercise 17\n",
    "##exercise 18"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
